{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/vangelis/.local/share/virtualenvs/machine-learning-msc-sNBHIkKd/lib/python3.8/site-packages (1.20.2)\n",
      "Requirement already satisfied: pandas in /home/vangelis/.local/share/virtualenvs/machine-learning-msc-sNBHIkKd/lib/python3.8/site-packages (1.2.4)\n",
      "Requirement already satisfied: sklearn in /home/vangelis/.local/share/virtualenvs/machine-learning-msc-sNBHIkKd/lib/python3.8/site-packages (0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/vangelis/.local/share/virtualenvs/machine-learning-msc-sNBHIkKd/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/vangelis/.local/share/virtualenvs/machine-learning-msc-sNBHIkKd/lib/python3.8/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/vangelis/.local/share/virtualenvs/machine-learning-msc-sNBHIkKd/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: scikit-learn in /home/vangelis/.local/share/virtualenvs/machine-learning-msc-sNBHIkKd/lib/python3.8/site-packages (from sklearn) (0.24.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/vangelis/.local/share/virtualenvs/machine-learning-msc-sNBHIkKd/lib/python3.8/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/vangelis/.local/share/virtualenvs/machine-learning-msc-sNBHIkKd/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.6.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/vangelis/.local/share/virtualenvs/machine-learning-msc-sNBHIkKd/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy pandas sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def remove_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Removes missing/corrupted values from a dataframe\n",
    "    :param df: The given dataframe\n",
    "    :return: The cleaned up dataframe\n",
    "    \"\"\"\n",
    "    df = df.replace(r'\\?', np.nan, regex=True)\n",
    "    return df.dropna()\n",
    "\n",
    "\n",
    "def binning(df: pd.DataFrame, columns: list) -> pd.DataFrame:\n",
    "    \"\"\"Transforms string categorical feature values to integers\n",
    "    :param df: The dataframe to process\n",
    "    :param columns: The columns that contain categorical values\n",
    "    :return: The normalized dataframe\n",
    "    \"\"\"\n",
    "    df[columns] = df[columns].apply(lambda x: pd.factorize(x)[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Naive Bayes model\n",
    "\n",
    "This Naive Bayes model accepts datasets that have mixed feature type, categorical & continuous. The model's engine\n",
    "estimates the prior probabilities $\\pi_k = p(C_k)$ for each given class & the densities\n",
    "$f(\\textbf{x})_k = p(\\textbf{x}|C_k)$ of each feature. For the categorical attributes the densities are yielded by\n",
    "dividing the count of each discrete value of the feature to the number of items that correspond to each class and for\n",
    "continuous attributes the densities are extracted by calculating Gaussian Distributions, estimating mu and sigma\n",
    "parameters. The parameters estimations are based to MLE (_Maximum Likelihood Estimation_).\n",
    "\n",
    "In order to predict, initially we estimate the posterior probability of the given data point by applying the following:\n",
    "$ \\hat{\\textbf{P}_k} = \\log \\left( p(C_k)\\prod_{i=1}^{n}(p(\\textbf{x}_{i}|C_k)) \\right)$\n",
    "\n",
    "Finally, the predicted class will be extracted by applying: $\\underset{k\\in {1,...,K}} {argmax} \\hat{\\textbf{P}_k}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CustomNaiveBayes:\n",
    "    def __init__(self):\n",
    "        self._labels = list()\n",
    "        self._priors = dict()\n",
    "        self._categorical_cols = list()\n",
    "        self._continuous_cols = list()\n",
    "        self._continuous_params = defaultdict(lambda: defaultdict(dict))\n",
    "        self._categorical_params = defaultdict(lambda: defaultdict(dict))\n",
    "\n",
    "    def __estimate_priors(self, Y: pd.DataFrame) -> None:\n",
    "        \"\"\"Estimates the prior probabilities for each class\n",
    "        :param Y: The train's dataset labels\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        y_np = Y.to_numpy()\n",
    "        labels, counts = np.unique(y_np, return_counts=True)\n",
    "        labels = dict(zip(labels, counts))\n",
    "        self._labels = labels.keys()\n",
    "        for label, count in labels.items():\n",
    "            self._priors.update({label: count / y_np.shape[0]})\n",
    "\n",
    "    def __parameter_estimation(self, X: pd.DataFrame, Y: pd.DataFrame) -> None:\n",
    "        \"\"\"Main method that estimates the density parameters, based on the type of each\n",
    "        feature (categorical or continuous)\n",
    "        :param X: The train data set points\n",
    "        :param Y: The train's dataset labels\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        y_np = Y.to_numpy()\n",
    "        labels_w_indexes = dict()\n",
    "        for label in self._labels:\n",
    "            indexes = np.where(y_np == label)\n",
    "            labels_w_indexes.update({label: indexes})\n",
    "\n",
    "        for column in X:\n",
    "            col_np = X[column].to_numpy()\n",
    "            if column in self._categorical_cols:\n",
    "                self.__categorical_parameter_estimation(col_np, labels_w_indexes, column)\n",
    "            elif column in self._continuous_cols:\n",
    "                self.__continuous_parameter_estimation(col_np, labels_w_indexes, column)\n",
    "\n",
    "    def __categorical_parameter_estimation(self, x_np: np.array, labels_w_indexes: dict, column_name: str) -> None:\n",
    "        \"\"\"Estimates the density parameters for categorical features\n",
    "        :param x_np: 1-D Numpy array that contains an entire feature column from the train dataframe\n",
    "        :param labels_w_indexes: Dictionary that contains the label associated with the index of each data point\n",
    "        :param column_name: The name of the feature\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        for label, indexes in labels_w_indexes.items():\n",
    "            x_np_split = np.take(x_np, indexes[0])\n",
    "            discrete_categorical_val, counts = np.unique(x_np_split, return_counts=True)\n",
    "            idx = 0\n",
    "            for val in discrete_categorical_val:\n",
    "                self._categorical_params[column_name][label][val] = counts[idx] / x_np_split.shape[0]\n",
    "                idx += 1\n",
    "\n",
    "    def __continuous_parameter_estimation(self, x_np: np.array, labels_w_indexes: dict, column_name: str) -> None:\n",
    "        \"\"\"Estimates the density parameters for continuous features\n",
    "        :param x_np: 1-D Numpy array that contains an entire feature column from the train dataframe\n",
    "        :param labels_w_indexes: Dictionary that contains the label associated with the index of each data point\n",
    "        :param column_name: The name of the feature\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        for label, indexes in labels_w_indexes.items():\n",
    "            x_np_split = np.take(x_np, indexes[0])\n",
    "            mu = np.mean(x_np_split)\n",
    "            sigma = np.std(x_np_split)\n",
    "            self._continuous_params[column_name][label]['mu'] = mu\n",
    "            self._continuous_params[column_name][label]['sigma'] = sigma\n",
    "\n",
    "    def __log_posterior(self, data_point: pd.Series, label: str):\n",
    "        \"\"\"Calculates the log posterior probability of a data point according to a label/class\n",
    "        :param data_point: The data point values\n",
    "        :param label: The class\n",
    "        :return: The log posterior probability\n",
    "        \"\"\"\n",
    "        densities = np.array([])\n",
    "        for col_name, value in data_point.items():\n",
    "            try:\n",
    "                density = None\n",
    "                if col_name in self._categorical_cols:\n",
    "                    density = self._categorical_params[col_name][label][value]\n",
    "                elif col_name in self._continuous_cols:\n",
    "                    density = self.__calculate_normal_dist(data_point[col_name],\n",
    "                                                           self._continuous_params[col_name][label]['mu'],\n",
    "                                                           self._continuous_params[col_name][label]['sigma'])\n",
    "                if density is not None:\n",
    "                    densities = np.append(densities, density)\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "        return np.log10(np.prod(densities) * self._priors[label])\n",
    "\n",
    "    @staticmethod\n",
    "    def __calculate_normal_dist(x: float, mu: float, sigma: float) -> float:\n",
    "        \"\"\"Calculates the normal distribution's value for a test data point given the values of continuous attributes\n",
    "        :param x: The value of the continuous attribute of the test data point\n",
    "        :param mu: The mu factor of the trained Gaussian Distribution\n",
    "        :param sigma: The sigma factor of the trained Gaussian Distribution\n",
    "        :return: The calculated value/probability\n",
    "        \"\"\"\n",
    "        return (2. * np.pi * sigma ** 2.) ** -.5 * np.exp(-.5 * (x - mu) ** 2. / sigma ** 2.)\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def __pretty_print(nested_dict: defaultdict, file) -> None:\n",
    "        \"\"\"Writes to a file a dict that contains the model's parameters\n",
    "        :param nested_dict: The dict to print\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        def to_dict(d: defaultdict) -> dict:\n",
    "            \"\"\"Converts a defaultdict to dict\n",
    "            :param d: The defaultdict\n",
    "            :return: The dict\n",
    "            \"\"\"\n",
    "            if not isinstance(d, dict):\n",
    "                return d\n",
    "            return {k: to_dict(v) for k, v in d.items()}\n",
    "        \n",
    "        pprint.pprint(to_dict(nested_dict), stream=file)\n",
    "\n",
    "    def __print_parameters(self) -> None:\n",
    "        \"\"\"Print the parameters of the trained model\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        with open('nb_parameters.txt', 'w') as f:\n",
    "            f.write(f'Priors\\n{self._priors}\\n')\n",
    "            f.write('\\nCategorical Parameters\\n')\n",
    "            self.__pretty_print(self._categorical_params, f)\n",
    "            f.write('\\nContinuous Parameters\\n')\n",
    "            self.__pretty_print(self._continuous_params, f)\n",
    "\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, Y: pd.DataFrame, categorical_cols: list, continuous_cols: list) -> None:\n",
    "        \"\"\"Fit/train method for a Gaussian model with mixed categorical & continuous attributes\n",
    "        :param X: The train data set points\n",
    "        :param Y: The train's dataset labels\n",
    "        :param categorical_cols: The column names that are categorical features\n",
    "        :param continuous_cols: The column names that are continuous features\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self._categorical_cols = categorical_cols\n",
    "        self._continuous_cols = continuous_cols\n",
    "        self.__estimate_priors(Y)\n",
    "        self.__parameter_estimation(X, Y)\n",
    "        self.__print_parameters()\n",
    "\n",
    "    def predict(self, X: pd.DataFrame) -> list:\n",
    "        \"\"\"Predicts a test set labels\n",
    "        :param X: The data points to predict\n",
    "        :return: List with the predicted labels\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for i, row in X.iterrows():\n",
    "            extracted_probabilities = list()\n",
    "            for label in self._labels:\n",
    "                extracted_probabilities.append((label, self.__log_posterior(row, label)))\n",
    "\n",
    "            predicted_class = max(extracted_probabilities, key=lambda item: item[1])[0]\n",
    "            predictions.append(predicted_class)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       age  workclass  fnlwgt  education  education-num  marital-status  \\\n",
      "0       50          0   83311          0             13               0   \n",
      "1       38          1  215646          1              9               1   \n",
      "2       53          1  234721          2              7               0   \n",
      "3       28          1  338409          0             13               0   \n",
      "4       37          1  284582          3             14               0   \n",
      "...    ...        ...     ...        ...            ...             ...   \n",
      "32555   27          1  257302          6             12               0   \n",
      "32556   40          1  154374          1              9               0   \n",
      "32557   58          1  151910          1              9               6   \n",
      "32558   22          1  201490          1              9               3   \n",
      "32559   52          5  287927          1              9               0   \n",
      "\n",
      "       occupation  relationship  race  sex  capital-gain  capital-loss  \\\n",
      "0               0             0     0    0             0             0   \n",
      "1               1             1     0    0             0             0   \n",
      "2               1             0     1    0             0             0   \n",
      "3               2             2     1    1             0             0   \n",
      "4               0             2     0    1             0             0   \n",
      "...           ...           ...   ...  ...           ...           ...   \n",
      "32555           9             2     0    1             0             0   \n",
      "32556           8             0     0    0             0             0   \n",
      "32557           4             4     0    1             0             0   \n",
      "32558           4             3     0    0             0             0   \n",
      "32559           0             2     0    1         15024             0   \n",
      "\n",
      "       hours-per-week  native-country  label  \n",
      "0                  13               0      0  \n",
      "1                  40               0      0  \n",
      "2                  40               0      0  \n",
      "3                  40               1      0  \n",
      "4                  40               0      0  \n",
      "...               ...             ...    ...  \n",
      "32555              38               0      0  \n",
      "32556              40               0      1  \n",
      "32557              40               0      0  \n",
      "32558              20               0      0  \n",
      "32559              40               0      1  \n",
      "\n",
      "[30161 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('dataset/adult.data')\n",
    "train_df.columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation',\n",
    "              'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n",
    "              'label']\n",
    "\n",
    "categorical_cols = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "                    'native-country', 'label']\n",
    "continuous_cols = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "\n",
    "train_df = remove_missing_values(train_df)\n",
    "train_df = binning(train_df, categorical_cols)\n",
    "print(train_df)\n",
    "\n",
    "train_X = train_df.loc[:, train_df.columns != 'label']\n",
    "train_Y = train_df['label']\n",
    "\n",
    "naive_bayes = CustomNaiveBayes()\n",
    "priors = naive_bayes.fit(train_X, train_Y, categorical_cols=categorical_cols, continuous_cols=continuous_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Train dataset data-points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-b8e0133de7e6>:92: RuntimeWarning: divide by zero encountered in log10\n",
      "  return np.log10(np.prod(densities) * self._priors[label])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.93      0.89     22653\n",
      "           1       0.71      0.51      0.60      7508\n",
      "\n",
      "    accuracy                           0.83     30161\n",
      "   macro avg       0.78      0.72      0.74     30161\n",
      "weighted avg       0.82      0.83      0.82     30161\n",
      "\n",
      "Predicting Test dataset data-points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-b8e0133de7e6>:92: RuntimeWarning: divide by zero encountered in log10\n",
      "  return np.log10(np.prod(densities) * self._priors[label])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.93      0.87     11359\n",
      "           1       0.62      0.35      0.44      3700\n",
      "\n",
      "    accuracy                           0.79     15059\n",
      "   macro avg       0.72      0.64      0.66     15059\n",
      "weighted avg       0.77      0.79      0.76     15059\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('dataset/adult.test')\n",
    "test_df.columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation',\n",
    "                   'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n",
    "                   'label']\n",
    "test_df = remove_missing_values(test_df)\n",
    "test_df = binning(test_df, categorical_cols)\n",
    "\n",
    "test_X = test_df.loc[:, test_df.columns != 'label']\n",
    "test_Y = test_df['label']\n",
    "\n",
    "print('Predicting Train dataset data-points')\n",
    "results = naive_bayes.predict(train_X)\n",
    "print(classification_report(train_Y, results))\n",
    "\n",
    "print('Predicting Test dataset data-points')\n",
    "results = naive_bayes.predict(test_X)\n",
    "print(classification_report(test_Y, results))\n",
    "\n",
    "with open('predictions.txt', 'w') as f:\n",
    "    f.write('\\n'.join(map(str, results)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}